{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52abf00-71aa-48ca-9c46-5db7111873e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6690f7d-e412-4490-ba3b-d06e083440dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "\n",
    "data = [\n",
    "    {\"Name\": \"Anil Kumar\", \"Age\": 29}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "display(df.select(\"Name\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d938cfd1-be32-416b-8e2f-9b7b7f7da665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "columns = [\"emp_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "#display(df.groupby(\"department\").sum(\"salary\"))\n",
    "display(df)\n",
    "display(df.groupby(\"department\").min(\"salary\"))\n",
    "display(df.groupby(\"department\").max(\"salary\"))\n",
    "display(df.groupby(\"department\").agg({\"salary\": \"sum\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "31832d71-3668-451f-ac5b-c5de2be583cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "columns = [\"emp_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\")\n",
    "df = df.withColumn(\"sum_salary\", sum(\"salary\").over(windowSpec))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5377f3c-1922-45dd-bcd8-84dc6dae2deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, max\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "    avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "\n",
    "display(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83267a57-124d-4e61-8b73-7cd7197372b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.filter(\"age > 30\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2c939407-339e-4854-ad06-a435c8a0c663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "columns = [\"emp_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "\n",
    "windowSpec = Window.partitionBy(\"state\")\n",
    "df = df.withColumn(\"state_total_salary\", sum(\"salary\").over(windowSpec))\n",
    "df = df.filter(\"salary > 90000\")\n",
    "display(df.select (\"emp_name\", \"state\", \"state_total_salary\", \"salary\").orderBy(\"state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6cf026ad-504b-4e89-9a36-7e7baebda26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "#Read the CSV file\n",
    "df = spark.read.csv(\"/FileStore/tables/iris_data.csv\")\n",
    "#df.show()\n",
    "\n",
    "#Rename the columns\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df = df.toDF(*columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "99195931-32bd-41cd-a38d-bd23e5ce83d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = df.agg(max(\"sepal_length\").alias(\"max_sepal_length\")).collect()[0]\n",
    "y = df.agg(max(\"petal_length\").alias(\"max_petal_length\")).collect()[0]\n",
    "display(x)\n",
    "display(y)\n",
    "\n",
    "\n",
    "result = \"sepal\" if x > y else \"petal\"\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2d5d035a-32ec-4add-acd9-0bd8ee0298a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", 10, \"5th\", 5000),\n",
    "    (\"Bob\", 11, \"6th\", 5500),\n",
    "    (\"Charlie\", 10, \"5th\", 5000),\n",
    "    (\"David\", 12, \"7th\", 6000),\n",
    "    (\"Eva\", 11, \"6th\", 5500),\n",
    "    (\"Frank\", 10, \"5th\", 5000),\n",
    "    (\"Grace\", 12, \"7th\", 6000),\n",
    "    (\"Hannah\", 11, \"6th\", 5500),\n",
    "    (\"Ivy\", 10, \"5th\", 5000),\n",
    "    (\"Jack\", 12, \"7th\", 6000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"grade\", \"fee\"]\n",
    "\n",
    "df_school = spark.createDataFrame(data, columns)\n",
    "#display(df_school)\n",
    "\n",
    "df_school = df_school.withColumn(\"age\", df_school[\"age\"].cast(\"int\"))\n",
    "df_school = df_school.withColumn(\"fee\", df_school[\"fee\"].cast(\"int\"))\n",
    "df_school = df_school.withColumn(\"fee\", df_school[\"fee\"] * 10)\n",
    "display(df_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0ac57a3a-1c9f-4417-b30a-d384a9e8c9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_school.withColumnsRenamed({\"name\": \"Student_Name\", \"grade\": \"Class\"})\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c419aaad-5281-465c-97a0-1e230d3fac6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "dataDF = [\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"M\", 3100),\n",
    "    ((\"Michael\", \"Rose\", \"\"), \"40288\", \"M\", 4300),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"M\", 1400),\n",
    "    ((\"Maria\", \"Anne\", \"Jones\"), \"39192\", \"F\", 5500),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", -1)\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StructType([\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"middle_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_data = spark.createDataFrame(dataDF, schema=schema)\n",
    "#display(df_data)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "schema2 = StructType([\n",
    "    StructField(\"f1name\",StringType()),\n",
    "    StructField(\"m1iddlename\",StringType()),\n",
    "    StructField(\"l1name\",StringType())])\n",
    " \n",
    "df = df_data.select(col(\"name\").cast(schema2))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a92d4660-003a-441d-9b9b-274e244e6b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])\n",
    "\n",
    "distinct_df = df.distinct()\n",
    "print(\"Distinct count: \" + str(distinct_df.count()))\n",
    "\n",
    "df = df.dropDuplicates()\n",
    "print(\"Distinct count: \" + str(df.count()))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe7c7b2d-3072-4a67-8c51-33b09365dc98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"Sales\", 3000, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (\"Michael\", \"Sales\", 4600, {\"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}),\n",
    "    (\"Robert\", \"Sales\", 4100, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (\"Maria\", \"Finance\", 3000, {\"address\": {\"city\": \"Seattle\", \"zip\": \"98101\"}}),\n",
    "    (\"James\", \"Sales\", 3000, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (\"Scott\", \"Finance\", 3300, {\"address\": {\"city\": \"Seattle\", \"zip\": \"98101\"}}),\n",
    "    (\"Jen\", \"Finance\", 3900, {\"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}),\n",
    "    (\"Jeff\", \"Marketing\", 3000, {\"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}),\n",
    "    (\"Kumar\", \"Marketing\", 2000, {\"address\": {\"city\": \"Seattle\", \"zip\": \"98101\"}}),\n",
    "    (\"Saif\", \"Sales\", 4100, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}})\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"Dept_name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"zip\", StringType(), True),\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c9e9b41e-26bc-45b6-9493-807a20945325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GroupBy Aggregations:\n",
    "# Calculate minimum, maximum, average, and total salary per department.\n",
    "\n",
    "from pyspark.sql.functions import min, max, avg, sum\n",
    "\n",
    "agg_df = df.groupBy(\"Dept_name\").agg(\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ")\n",
    "\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e324ebc6-c667-4e89-8363-876273e4118f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Window Functions (Over):\n",
    "#Assign a dense rank of salary within each department using Window.partitionBy.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Dept_name\").orderBy(\"salary\")\n",
    "window_df = df.withColumn(\"dense_rank\", dense_rank().over(windowSpec))\n",
    "display(window_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f2820d4c-f941-4195-979b-2be20a28a8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Where Clause:\n",
    "#Filter out employees who earn more than 3000 and are in the Sales department.\n",
    "\n",
    "#display(df)\n",
    "\n",
    "filter_df = df.filter((col(\"salary\") > 3000) & (col(\"Dept_name\") == \"Sales\"))\n",
    "display(filter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7427ef4a-915e-44cc-b455-9332e8174b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#collect()[0][0]:\n",
    "#Retrieve the maximum salary in the company using collect()[0][0].\n",
    "\n",
    "#display(df)\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "max_sal_df = df.agg(max(\"salary\").alias(\"max_salary\")).collect()[0][0]\n",
    "display(max_sal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "70553d5c-2c5e-4941-a5c2-fd2028988b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If-Else Logic:\n",
    "#Create a new column called \"salary_level\":\n",
    "#\"Low\" if salary < 3000\n",
    "#\"Medium\" if salary between 3000â€“4000\n",
    "#\"High\" if salary > 4000\n",
    "\n",
    "#display(df)\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "if_df = df.withColumn(\"Salary_level\",\n",
    "                      when(col(\"salary\") < 3000, \"Low\")\n",
    "                      .when((col(\"salary\") >= 3000) & (col(\"salary\") <= 4000), \"Medium\")\n",
    "                      .otherwise(\"High\"))\n",
    "display(if_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7ace1b86-38b5-4cb5-b2fc-868f87f960fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#withColumnRenamed:\n",
    "#Rename \"employee_name\" to \"name\".\n",
    "\n",
    "#display(df)\n",
    "\n",
    "rename_df = df.withColumnRenamed(\"emp_name\", \"name\")\n",
    "display(rename_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "138e262b-71b8-4e7d-87ad-7a1d82535681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Update Nested Schema:\n",
    "#Update nested \"address.city\" to uppercase using withField or explode-nested technique.\n",
    "\n",
    "#display(df)\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "nested_df = df.withfield(\"address.city\", upper(col(\"address.city\")))\n",
    "\n",
    "display(nested_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6abe13f6-1201-4a12-af6f-7001c1a9f38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Drop Duplicates / Distinct:\n",
    "#Remove duplicate rows (exact matches).\n",
    "\n",
    "#display(df)\n",
    "\n",
    "distinct_df = df.distinct()\n",
    "print(\"Distinct count: \" + str(distinct_df.count()))\n",
    "\n",
    "drop_duplicates_df = df.dropDuplicates()\n",
    "print(\"Distinct count: \" + str(drop_duplicates_df.count()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 2025-07-08",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
