{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a88cc23-e4c8-496d-ba93-43ef6f483f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "838c9149-8ce6-4044-b0c8-54fd7edf8b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, BooleanType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"FORECAST_DATE\", DateType(), True),\n",
    "  StructField(\"PRODUCT_CODE\", FloatType(), True),\n",
    "  StructField(\"CHAIN_CODE\", StringType(), True)\n",
    "])  \n",
    "\n",
    "df=spark.read.csv(\"/FileStore/tables/Duplicate_RELEX_Data.csv\",header=True, schema=schema)\n",
    "display(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe97440c-c5de-4a28-bd1c-bb822ae2275f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": 1,\n",
    "  \"name\": \"John Doe\",\n",
    "  \"address\": {\n",
    "    \"street\": \"123 Main St\",\n",
    "    \"city\": \"New York\",\n",
    "    \"zip\": \"10001\"\n",
    "  },\n",
    "  \"contacts\": [\n",
    "    {\n",
    "      \"type\": \"email\",\n",
    "      \"value\": \"john@example.com\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"phone\",\n",
    "      \"value\": \"123-456-7890\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "38b681b6-a9a2-4248-9cd3-d5e1e25ab944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"zip\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"contacts\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"value\", StringType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", {\"street\": \"123 Main St\", \"city\": \"New York\", \"zip\": \"10001\"}, [\n",
    "        {\"type\": \"email\", \"value\": \"john@example.com\"},\n",
    "        {\"type\": \"phone\", \"value\": \"123-456-7890\"}\n",
    "    ])\n",
    "], schema)\n",
    "\n",
    "display(df)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e98df11-2c0f-4c8d-9ac3-bfb4b3f77a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"planguage\", ArrayType(StringType()), True),\n",
    "    StructField(\"olanguage\", ArrayType(StringType()), True),\n",
    "    StructField(\"city1\", StringType(), True),\n",
    "    StructField(\"city2\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    "(\"Michael,,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    "(\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "display(df)\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "14f1b425-3bd3-430e-bd5a-1152bc15d69a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(df.FullName,explode(df.planguage)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dcc9d967-3603-418d-b20d-9ed36266bee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(df.FullName,\",\").alias(\"FullName\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1a8a6cb0-5326-4074-b962-3ff4971c6a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataDictionary = [\n",
    "('James',{'hair':'black'}),\n",
    "('Michael',{'hair':'brown'}),\n",
    "('Robert',{'hair':'red'}),\n",
    "('Washington',{'hair':'grey'}),\n",
    "('Jefferson',{'hair':'brown'})\n",
    "]\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"properties\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary, schema=schema)\n",
    " \n",
    "df.display()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924d0bc5-79db-4767-a993-3dde9b44fd65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType\n",
    "arrayCol = ArrayType(StringType(),False)\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    " \n",
    " \n",
    "data = [\n",
    "(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    "(\"Michael,,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    "(\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\"),\n",
    "  (\"Nitesh,sharma\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    " \n",
    " \n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    " \n",
    " \n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
    "    .alias(\"array_contains\")).show()\n",
    "\n",
    "df.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff81c38-f561-40a7-a945-87db6e7e65d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\", 3)], [\"letter\", \"number\"])\n",
    "display(df1)\n",
    "\n",
    "rdd1 = df1.rdd"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 2025-07-04",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
