{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d778b0-c764-47f4-ba26-94581400a427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d7d10123-ac3d-4f51-b463-88db08688917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Order by & Sort\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    " \n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
    "df.orderBy(df.department.asc(),df.state.asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9e11c2f7-f69f-4174-957b-7ce589aafe8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Union\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    " \n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    " \n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    " \n",
    "union_df = df.union(df2).distinct()\n",
    "union_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "468f0cc6-f153-4e65-ad17-1ab2b1a7168b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#UnionByName\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    " \n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    " \n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "unionbyname_df = df.unionByName(df2).distinct().orderBy(col(\"employee_name\").asc())\n",
    "unionbyname_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "41c0fd02-e61a-40ec-8185-26071bc628ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Joins - inner, left, right, leftanti, leftsemi, cross\n",
    "\n",
    "data_employees = [\n",
    "    (1, \"James\", \"Sales\", \"NY\"),\n",
    "    (2, \"Michael\", \"Sales\", \"NY\"),\n",
    "    (3, \"Robert\", \"Sales\", \"CA\"),\n",
    "    (4, \"Maria\", \"Finance\", \"CA\"),\n",
    "    (5, \"Jen\", \"Finance\", \"NY\"),\n",
    "    (6, \"Jeff\", \"Marketing\", \"CA\"),\n",
    "    (7, \"Kumar\", \"Marketing\", \"NY\")\n",
    "]\n",
    "columns_employees = [\"emp_id\", \"employee_name\", \"department\", \"state\"]\n",
    "employees_df = spark.createDataFrame(data_employees, columns_employees)\n",
    "\n",
    "data_departments = [\n",
    "    (\"Sales\", \"John\"),\n",
    "    (\"Finance\", \"Samantha\"),\n",
    "    (\"Marketing\", \"Steve\"),\n",
    "    (\"IT\", \"Nancy\")\n",
    "]\n",
    "columns_departments = [\"department\", \"manager\"]\n",
    "departments_df = spark.createDataFrame(data_departments, columns_departments)\n",
    "\n",
    "data_salaries = [\n",
    "    (1, 90000, 10000),\n",
    "    (2, 86000, 20000),\n",
    "    (3, 81000, 23000),\n",
    "    (4, 90000, 23000),\n",
    "    (5, 79000, 15000),\n",
    "    (6, 80000, 18000),\n",
    "    (7, 91000, 21000)\n",
    "]\n",
    "columns_salaries = [\"emp_id\", \"salary\", \"bonus\"]\n",
    "salaries_df = spark.createDataFrame(data_salaries, columns_salaries)\n",
    "\n",
    "data_states = [\n",
    "    (\"NY\", \"New York\"),\n",
    "    (\"CA\", \"California\"),\n",
    "    (\"TX\", \"Texas\")\n",
    "]\n",
    "columns_states = [\"state\", \"state_name\"]\n",
    "states_df = spark.createDataFrame(data_states, columns_states)\n",
    "\n",
    "\n",
    "emp_dept_df = employees_df.join(departments_df, employees_df.department == departments_df.department, \"inner\")\n",
    "emp_dept_sal_df = emp_dept_df.join(salaries_df, emp_dept_df.emp_id == salaries_df.emp_id, \"inner\")\n",
    "emp_dept_sal_state_df = emp_dept_sal_df.join(states_df, emp_dept_sal_df.state == states_df.state, \"inner\")\n",
    "\n",
    "#display(emp_dept_sal_state_df)\n",
    "\n",
    "final_df = emp_dept_sal_state_df.select(employees_df.employee_name, departments_df.department,departments_df.manager, states_df.state, salaries_df.salary, salaries_df.bonus).orderBy(employees_df.employee_name)\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c0635f75-acb0-4794-810f-e5a7b3807ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for loop\n",
    "sample_data = [(\"Alice\", 28, \"Engineer\"),\n",
    "               (\"Bob\", 34, \"Doctor\"),\n",
    "               (\"Cathy\", 29, \"Teacher\"),\n",
    "               (\"David\", 42, \"Artist\")]\n",
    "\n",
    "columns = [\"name\",\"age\",\"profession\"]\n",
    "\n",
    "df = spark.createDataFrame(data = sample_data, schema= columns)\n",
    "\n",
    "for i in df.collect():\n",
    "    print(i.name, i.age, i.profession )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b8a169a-62fc-479c-8f8b-26194f091627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import functions as F\n",
    " \n",
    " \n",
    "# Create largefrom pyspark.sql import functions as F\n",
    "#dataset\n",
    "df = spark.range(0, 10_000_000)\n",
    "df_transformed = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])\n",
    "# First action: count()\n",
    "start = time.time()\n",
    "df_transformed.count()\n",
    "print(\"Without Cache - First count done in:\", time.time() - start, \"seconds\")\n",
    " \n",
    " \n",
    " \n",
    "# Second action: sum()\n",
    "start = time.time()\n",
    "df_transformed.agg(F.sum(\"square\")).show()\n",
    "print(\"Without Cache - Second aggregation done in:\", time.time() - start, \"seconds\")\n",
    " \n",
    " \n",
    " \n",
    "# Create large dataset\n",
    "df = spark.range(0, 10_000_000)\n",
    "df_transformed = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])\n",
    "# Apply caching\n",
    "df_transformed.cache()\n",
    "# First action: count() triggers caching\n",
    "start = time.time()\n",
    "df_transformed.count() #first action is slower (because it's doing both compute + caching).\n",
    "print(\"With Cache - First count done in:\", time.time() - start, \"seconds\")\n",
    " \n",
    "# Second action: sum() uses cached data\n",
    "start = time.time()\n",
    "df_transformed.agg(F.sum(\"square\")).show()\n",
    "print(\"With Cache - Second aggregation done in:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cf82c883-1607-4e13-a844-349b1d266fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr\n",
    "\n",
    "convertCase(\"anil kumar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e1a58f06-9f5e-4047-a183-8e3c3136f007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN global_temp\").show()\n",
    " \n",
    "spark.catalog.listTables()  # Lists tables and views in the current database\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8d4551e7-8a35-41bb-b83e-e2339dca87a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", age=23, salary=5000, dept=\"HR\"),\n",
    "    Row(id=2, name=\"Bob\", age=35, salary=7000, dept=\"IT\"),\n",
    "    Row(id=3, name=\"Charlie\", age=30, salary=4000, dept=\"HR\"),\n",
    "    Row(id=4, name=\"David\", age=45, salary=9000, dept=\"Finance\"),\n",
    "    Row(id=5, name=\"Eve\", age=28, salary=6000, dept=\"IT\"),\n",
    "    Row(id=6, name=\"Frank\", age=50, salary=10000, dept=\"Finance\"),\n",
    "    Row(id=7, name=\"Grace\", age=27, salary=3000, dept=\"HR\"),\n",
    "    Row(id=8, name=\"Heidi\", age=32, salary=8000, dept=\"IT\"),\n",
    "    Row(id=9, name=\"Ivan\", age=41, salary=9500, dept=\"Finance\"),\n",
    "    Row(id=10, name=\"Judy\", age=29, salary=5500, dept=\"HR\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "#display(df)\n",
    "\n",
    "case_df = df.withColumn(\"Salary_category\", when (col(\"salary\") > 5000, \"High\")\n",
    "                                            .when ((col(\"salary\") > 5000) & (col(\"salary\") < 8000), \"Medium\" ).otherwise(\"Low\"))\n",
    "display(case_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e699f44-e275-45c1-b8b4-09038e8aeb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data with nulls\n",
    "data = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 30), (None, 40)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "#display(df)\n",
    "\n",
    "# Replace nulls with a default value\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "case_df = df.withColumn(\"age\", when(col(\"age\").isNull(), 89)\n",
    "                        .when(col(\"age\")>50, 60)\n",
    "                        .otherwise(col(\"age\")))\n",
    "display(case_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8507836553201800,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 2025-07-09",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
