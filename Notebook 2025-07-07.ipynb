{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1f1542-20d2-4065-8ea8-3cdee0cef798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a2eb0ee4-6330-499d-a993-57581fa74489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Anil Kumar\", 29), (\"Dinesh Vimal\", 36), (\"Vinay Varma\", 42), (\"Anantha chary\", 48)]\n",
    "df = spark.createDataFrame(data, [\"Name\",\"Age\"])\n",
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4d3cc5e0-bb1c-4216-a620-20a28a6be6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [Row(Name=\"Anil Kumar\", Age=29), Row(Name=\"Dinesh Vimal\", Age=36), Row(Name=\"Vinay Varma\", Age=42), Row(Name=\"Anantha chary\", Age=48)]\n",
    "df = spark.createDataFrame(data, [\"Name\",\"Age\"])\n",
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c1547291-67ba-4109-8c4b-8e44a62bec59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(\"Anil Kumar\",29), (\"Dinesh Vimal\", 36), (\"Vinay Varma\",42), (\"Anantha chary\", 48)]\n",
    "schema = [\"Name\",\"Age\"]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "#df.show()\n",
    "\n",
    "#df.select(col(\"Name\")).show()\n",
    "\n",
    "#df.select(*df.columns).show()\n",
    "\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "79e83e12-ba4c-4c0d-98c2-4bbf409d8d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Anil Kumar\", 29, \"M\", \"India\", \"Engineer\", 50000, \"Single\", \"Bangalore\", \"o+\", \"Cricket\", 5.8, 70, \"No\", \"None\"),\n",
    "    (\"Dinesh Vimal\", 36, \"M\", \"India\", \"Manager\", 150000, \"Married\", \"Chennai\", \"B+\", \"Football\", 5.9, 75, \"No\", \"None\"),\n",
    "    (\"Vinay Varma\", 42, \"M\", \"India\", \"Director\", 150000, \"Married\", \"Hyderabad\", \"O+\", \"Tennis\", 6.0, 80, \"Yes\", \"None\"),\n",
    "    (\"Anantha chary\", 48, \"M\", \"India\", \"VP\", 180000, \"Married\", \"Delhi\", \"AB+\", \"Golf\", 5.7, 78, \"Yes\", \"Hypertension\")\n",
    "]\n",
    "schema = [\n",
    "    \"Name\", \"Age\", \"Gender\", \"Country\", \"Occupation\", \"Salary\", \"MaritalStatus\", \"City\", \"BloodGroup\", \"Hobby\",\n",
    "    \"Height\", \"Weight\", \"Smoker\", \"MedicalCondition\"\n",
    "]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "#display(df.select(df.columns[:3]))\n",
    "#display(df.select(df.columns[0:3]))\n",
    "#display(df.limit(3))\n",
    "display(df.select(df.columns[:3])).display(df.limit(3))\n",
    "#display(df.select(\"Name\").orderBy(\"Name\"))\n",
    "#display(df.select(\"*\").orderBy(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5021c84-ddcd-42e1-91d2-83be7b88ae24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(\"file://\"+SparkFiles.get(\"iris.data\"), header=False, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f86a86e7-d503-4c57-8da6-115d4335e812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94b173ab-385e-4342-8971-db8b135fd097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = [f.path for f in dbutils.fs.ls(\"/FileStore/tables/\") if f.path.endswith('.csv')]\n",
    "df_multi = spark.read.csv(files, header=True, inferSchema=True)\n",
    "display(df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8ed0f429-7bd1-4759-9587-db6008abd1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"cresol.in\").getOrCreate()\n",
    "\n",
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "\n",
    "df=spark.createDataFrame(address,['Id','Address','State'])\n",
    "#display(df)\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df = df.withColumn('Address', regexp_replace('Address', 'Rd', 'Road'))\n",
    "#display(df)\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = df.withColumn('City', lit('CE'))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aec2dc2-e74d-4615-a3c3-1b705c4e96d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(name=\"Alice\", age=28, gender=\"F\", salary=50000),\n",
    "    Row(name=\"Bob\", age=35, gender=\"M\", salary=45000),\n",
    "    Row(name=\"Charlie\", age=22, gender=\"M\", salary=38000),\n",
    "    Row(name=\"Diana\", age=30, gender=\"F\", salary=None),\n",
    "    Row(name=\"Eve\", age=None, gender=\"F\", salary=42000),\n",
    "    Row(name=\"Frank\", age=24, gender=\"M\", salary=39000),\n",
    "    Row(name=\"Grace\", age=27, gender=\"F\", salary=41000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# 1. Equal / Not equal\n",
    "#df = df.filter(col(\"gender\") == \"M\")\n",
    "#df = df.filter(col(\"gender\") != \"M\")\n",
    "#display(df)\n",
    "\n",
    "# 2. Greater than / Less than\n",
    "#df.filter(col(\"age\") > 30).show()\n",
    "#df.filter(col(\"salary\") < 50000).show()\n",
    " \n",
    " \n",
    "# 3. Greater than or equal / Less than or equal\n",
    "#df.filter(col(\"age\") >= 25).show()\n",
    "#df.filter(col(\"salary\") <= 40000).show()\n",
    " \n",
    "# 4. IN / NOT IN\n",
    "#df.filter(col(\"name\").isin(\"Alice\", \"Bob\")).show()\n",
    "#df.filter(~col(\"name\").isin(\"Charlie\", \"Diana\")).show()\n",
    " \n",
    "# 5. BETWEEN\n",
    "#df.filter(col(\"age\").between(25, 30)).show()\n",
    " \n",
    "# 6. NULL / NOT NULL\n",
    "#df.filter(col(\"age\").isNull()).show()\n",
    "#df.filter(col(\"salary\").isNotNull()).show()\n",
    " \n",
    " \n",
    "# 7. Startswith / Endswith / Contains\n",
    "#df.filter(col(\"name\").startswith(\"A\")).show()\n",
    "#df.filter(col(\"name\").endswith(\"a\")).show()\n",
    "#df.filter(col(\"name\").contains(\"ar\")).show()\n",
    " \n",
    "# 8. Multiple conditions (AND / OR)\n",
    "#df.filter((col(\"gender\") == \"F\") & (col(\"age\") > 25)).show()\n",
    "#df.filter((col(\"age\") < 25) | (col(\"salary\") < 40000)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b28cd8c-d8bc-402f-ad0e-2b0babf23c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "\n",
    "#Read the CSV file\n",
    "df = spark.read.csv(\"/FileStore/tables/iris_data.csv\")\n",
    "#df.show()\n",
    "\n",
    "#Rename the columns\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df = df.toDF(*columns)\n",
    "#df.show()\n",
    "\n",
    "# Use Select to pick only specific columns\n",
    "#df = df.select(\"sepal_length\",\"sepal_width\")\n",
    "#df = df.select(\"*\")\n",
    "#df.show()\n",
    "\n",
    "#Add a new column `sepal_area` using `withColumn`\n",
    "df = df.withColumn(\"sepal_area\", (col(\"sepal_length\").cast(\"double\") * col(\"sepal_width\").cast(\"double\")))\n",
    "\n",
    "#Filter rows where class is 'Iris-setosa' and sepal_length > 5\n",
    "#df = df.filter((col(\"sepal_length\") > '5.0') & (col(\"class\") == 'Iris-setosa'))\n",
    "#display(df)\n",
    "\n",
    "#Combine all â€“ filter class 'Iris-virginica' with petal_width > 2, create petal_area, and select\n",
    "df = df.filter((col(\"petal_width\") > '2.0') & (col(\"class\") == 'Iris-virginica')).withColumn(\"petal_area\", lit(\"Test\"))\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 2025-07-07",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
